{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:27:35.884246Z",
     "start_time": "2025-04-29T05:27:33.494703Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision.models import densenet201, DenseNet201_Weights\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.optim import  AdamW\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69647398c36c4ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:27:36.052202Z",
     "start_time": "2025-04-29T05:27:36.036202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc27bcc61a28c73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:27:36.067204Z",
     "start_time": "2025-04-29T05:27:36.063073Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = ImageFolder(\"./dataset/\", transform=DenseNet201_Weights.IMAGENET1K_V1.transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b87f6f889c4f93b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:27:36.078721Z",
     "start_time": "2025-04-29T05:27:36.073716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adobo', 'inasal_na_manok', 'sinigang', 'sisig', 'tortang_talong']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3141cd04ee92d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:27:36.089649Z",
     "start_time": "2025-04-29T05:27:36.086553Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8c63b26fbad8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:27:36.459594Z",
     "start_time": "2025-04-29T05:27:36.101921Z"
    }
   },
   "outputs": [],
   "source": [
    "vision_model = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n",
    "vision_model.classifier = nn.Linear(vision_model.classifier.in_features, 5)\n",
    "vision_model = vision_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22edbc2b5fc641db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:27:36.474989Z",
     "start_time": "2025-04-29T05:27:36.469989Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(vision_model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3ba343efc270e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:32:38.636422Z",
     "start_time": "2025-04-29T05:27:36.484946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoc: 0, Step: 0, loss: 1.6202\n",
      "Epoc: 0, Step: 10, loss: 0.9668\n",
      "Epoc: 0, Step: 20, loss: 0.6742\n",
      "Epoc: 0, Step: 30, loss: 0.3655\n",
      "Epoc: 0, Step: 40, loss: 0.1265\n",
      "Epoc: 0, Step: 50, loss: 0.1020\n",
      "Epoc: 0, Step: 60, loss: 0.6183\n",
      "Epoc: 1, Step: 0, loss: 0.0782\n",
      "Epoc: 1, Step: 10, loss: 0.2147\n",
      "Epoc: 1, Step: 20, loss: 0.0299\n",
      "Epoc: 1, Step: 30, loss: 0.0547\n",
      "Epoc: 1, Step: 40, loss: 0.0696\n",
      "Epoc: 1, Step: 50, loss: 0.1879\n",
      "Epoc: 1, Step: 60, loss: 0.0172\n",
      "Epoc: 2, Step: 0, loss: 0.0152\n",
      "Epoc: 2, Step: 10, loss: 0.1451\n",
      "Epoc: 2, Step: 20, loss: 0.0943\n",
      "Epoc: 2, Step: 30, loss: 0.0247\n",
      "Epoc: 2, Step: 40, loss: 0.0729\n",
      "Epoc: 2, Step: 50, loss: 0.0128\n",
      "Epoc: 2, Step: 60, loss: 0.0468\n",
      "Epoc: 3, Step: 0, loss: 0.1051\n",
      "Epoc: 3, Step: 10, loss: 0.0976\n",
      "Epoc: 3, Step: 20, loss: 0.0766\n",
      "Epoc: 3, Step: 30, loss: 0.0746\n",
      "Epoc: 3, Step: 40, loss: 0.0781\n",
      "Epoc: 3, Step: 50, loss: 0.1992\n",
      "Epoc: 3, Step: 60, loss: 0.0312\n",
      "Epoc: 4, Step: 0, loss: 0.0235\n",
      "Epoc: 4, Step: 10, loss: 0.0968\n",
      "Epoc: 4, Step: 20, loss: 0.0191\n",
      "Epoc: 4, Step: 30, loss: 0.0072\n",
      "Epoc: 4, Step: 40, loss: 0.0116\n",
      "Epoc: 4, Step: 50, loss: 0.0120\n",
      "Epoc: 4, Step: 60, loss: 0.0047\n",
      "Epoc: 5, Step: 0, loss: 0.0180\n",
      "Epoc: 5, Step: 10, loss: 0.0130\n",
      "Epoc: 5, Step: 20, loss: 0.0078\n",
      "Epoc: 5, Step: 30, loss: 0.0307\n",
      "Epoc: 5, Step: 40, loss: 0.0068\n",
      "Epoc: 5, Step: 50, loss: 0.0506\n",
      "Epoc: 5, Step: 60, loss: 0.0501\n",
      "Epoc: 6, Step: 0, loss: 0.0063\n",
      "Epoc: 6, Step: 10, loss: 0.0509\n",
      "Epoc: 6, Step: 20, loss: 0.0846\n",
      "Epoc: 6, Step: 30, loss: 0.0058\n",
      "Epoc: 6, Step: 40, loss: 0.0095\n",
      "Epoc: 6, Step: 50, loss: 0.0053\n",
      "Epoc: 6, Step: 60, loss: 0.0160\n",
      "Epoc: 7, Step: 0, loss: 0.0143\n",
      "Epoc: 7, Step: 10, loss: 0.0087\n",
      "Epoc: 7, Step: 20, loss: 0.0390\n",
      "Epoc: 7, Step: 30, loss: 0.0109\n",
      "Epoc: 7, Step: 40, loss: 0.0075\n",
      "Epoc: 7, Step: 50, loss: 0.0596\n",
      "Epoc: 7, Step: 60, loss: 0.0079\n",
      "Epoc: 8, Step: 0, loss: 0.0083\n",
      "Epoc: 8, Step: 10, loss: 0.0036\n",
      "Epoc: 8, Step: 20, loss: 0.0187\n",
      "Epoc: 8, Step: 30, loss: 0.0463\n",
      "Epoc: 8, Step: 40, loss: 0.1201\n",
      "Epoc: 8, Step: 50, loss: 0.1459\n",
      "Epoc: 8, Step: 60, loss: 0.0027\n",
      "Epoc: 9, Step: 0, loss: 0.0217\n",
      "Epoc: 9, Step: 10, loss: 0.0085\n",
      "Epoc: 9, Step: 20, loss: 0.0078\n",
      "Epoc: 9, Step: 30, loss: 0.0168\n",
      "Epoc: 9, Step: 40, loss: 0.0045\n",
      "Epoc: 9, Step: 50, loss: 0.0058\n",
      "Epoc: 9, Step: 60, loss: 0.0036\n",
      "Epoc: 10, Step: 0, loss: 0.0063\n",
      "Epoc: 10, Step: 10, loss: 0.1141\n",
      "Epoc: 10, Step: 20, loss: 0.0064\n",
      "Epoc: 10, Step: 30, loss: 0.0498\n",
      "Epoc: 10, Step: 40, loss: 0.0026\n",
      "Epoc: 10, Step: 50, loss: 0.0056\n",
      "Epoc: 10, Step: 60, loss: 0.0058\n",
      "Epoc: 11, Step: 0, loss: 0.0033\n",
      "Epoc: 11, Step: 10, loss: 0.0033\n",
      "Epoc: 11, Step: 20, loss: 0.0071\n",
      "Epoc: 11, Step: 30, loss: 0.0088\n",
      "Epoc: 11, Step: 40, loss: 0.0036\n",
      "Epoc: 11, Step: 50, loss: 0.0045\n",
      "Epoc: 11, Step: 60, loss: 0.0030\n",
      "Epoc: 12, Step: 0, loss: 0.0544\n",
      "Epoc: 12, Step: 10, loss: 0.0027\n",
      "Epoc: 12, Step: 20, loss: 0.0251\n",
      "Epoc: 12, Step: 30, loss: 0.0027\n",
      "Epoc: 12, Step: 40, loss: 0.0202\n",
      "Epoc: 12, Step: 50, loss: 0.3396\n",
      "Epoc: 12, Step: 60, loss: 0.0225\n",
      "Epoc: 13, Step: 0, loss: 0.0141\n",
      "Epoc: 13, Step: 10, loss: 0.0304\n",
      "Epoc: 13, Step: 20, loss: 0.0319\n",
      "Epoc: 13, Step: 30, loss: 0.0049\n",
      "Epoc: 13, Step: 40, loss: 0.0521\n",
      "Epoc: 13, Step: 50, loss: 0.0056\n",
      "Epoc: 13, Step: 60, loss: 0.0026\n",
      "Epoc: 14, Step: 0, loss: 0.0043\n",
      "Epoc: 14, Step: 10, loss: 0.0037\n",
      "Epoc: 14, Step: 20, loss: 0.0054\n",
      "Epoc: 14, Step: 30, loss: 0.0014\n",
      "Epoc: 14, Step: 40, loss: 0.0026\n",
      "Epoc: 14, Step: 50, loss: 0.0051\n",
      "Epoc: 14, Step: 60, loss: 0.0027\n",
      "Epoc: 15, Step: 0, loss: 0.0234\n",
      "Epoc: 15, Step: 10, loss: 0.0203\n",
      "Epoc: 15, Step: 20, loss: 0.0046\n",
      "Epoc: 15, Step: 30, loss: 0.0022\n",
      "Epoc: 15, Step: 40, loss: 0.0014\n",
      "Epoc: 15, Step: 50, loss: 0.0076\n",
      "Epoc: 15, Step: 60, loss: 0.0019\n",
      "Epoc: 16, Step: 0, loss: 0.0585\n",
      "Epoc: 16, Step: 10, loss: 0.0047\n",
      "Epoc: 16, Step: 20, loss: 0.0019\n",
      "Epoc: 16, Step: 30, loss: 0.0010\n",
      "Epoc: 16, Step: 40, loss: 0.0024\n",
      "Epoc: 16, Step: 50, loss: 0.0068\n",
      "Epoc: 16, Step: 60, loss: 0.0043\n",
      "Epoc: 17, Step: 0, loss: 0.0016\n",
      "Epoc: 17, Step: 10, loss: 0.0018\n",
      "Epoc: 17, Step: 20, loss: 0.0007\n",
      "Epoc: 17, Step: 30, loss: 0.0016\n",
      "Epoc: 17, Step: 40, loss: 0.0086\n",
      "Epoc: 17, Step: 50, loss: 0.0204\n",
      "Epoc: 17, Step: 60, loss: 0.0317\n",
      "Epoc: 18, Step: 0, loss: 0.0055\n",
      "Epoc: 18, Step: 10, loss: 0.0075\n",
      "Epoc: 18, Step: 20, loss: 0.0024\n",
      "Epoc: 18, Step: 30, loss: 0.0045\n",
      "Epoc: 18, Step: 40, loss: 0.0014\n",
      "Epoc: 18, Step: 50, loss: 0.5547\n",
      "Epoc: 18, Step: 60, loss: 0.0789\n",
      "Epoc: 19, Step: 0, loss: 0.0234\n",
      "Epoc: 19, Step: 10, loss: 0.0624\n",
      "Epoc: 19, Step: 20, loss: 0.0289\n",
      "Epoc: 19, Step: 30, loss: 0.0367\n",
      "Epoc: 19, Step: 40, loss: 0.2948\n",
      "Epoc: 19, Step: 50, loss: 0.0197\n",
      "Epoc: 19, Step: 60, loss: 0.0247\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "for e in range(epoch):\n",
    "    vision_model.train()\n",
    "    for i, (img, label) in enumerate(dataloader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = vision_model(img)\n",
    "        loss = criterion(y_pred, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoc: {e}, Step: {i}, loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf37aa5f275c2c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:32:39.038088Z",
     "start_time": "2025-04-29T05:32:38.743567Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "f1_metric = MulticlassF1Score(num_classes=5, average='macro').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0fd1018470e8151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:32:48.268595Z",
     "start_time": "2025-04-29T05:32:39.075094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vision_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (img, label) in enumerate(dataloader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        y_pred = vision_model(img)\n",
    "        y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "        f1_metric.update(y_pred_class, label)\n",
    "\n",
    "print(f1_metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c750e8cbf99cc20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:32:59.423401Z",
     "start_time": "2025-04-29T05:32:59.266894Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(vision_model.state_dict(), f\"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cc4eb5a75ea8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:32:58.071531Z",
     "start_time": "2025-04-29T05:32:48.513966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Predicted class distribution: {0: 101, 1: 100, 2: 101, 3: 100, 4: 100}\n",
      "🏷️ True label distribution: {0: 101, 1: 100, 2: 101, 3: 100, 4: 100}\n",
      "📌 Sample predictions: [3, 0, 1, 1, 4, 1, 4, 3, 3, 3]\n",
      "📌 Sample labels:      [3, 0, 1, 1, 4, 1, 4, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "vision_model.eval()\n",
    "with torch.no_grad():\n",
    "    for img, label in dataloader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        y_pred = vision_model(img)\n",
    "        y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "        all_preds.append(y_pred_class.cpu())\n",
    "        all_labels.append(label.cpu())\n",
    "\n",
    "# Stack and analyze\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "# Print unique prediction distribution\n",
    "unique_preds, counts = all_preds.unique(return_counts=True)\n",
    "print(\"Predicted class distribution:\", dict(zip(unique_preds.tolist(), counts.tolist())))\n",
    "\n",
    "# Print label distribution too\n",
    "unique_labels, label_counts = all_labels.unique(return_counts=True)\n",
    "print(\"True label distribution:\", dict(zip(unique_labels.tolist(), label_counts.tolist())))\n",
    "\n",
    "# Sample predicted and true values\n",
    "print(\"Sample predictions:\", all_preds[:10].tolist())\n",
    "print(\"Sample labels:     \", all_labels[:10].tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
